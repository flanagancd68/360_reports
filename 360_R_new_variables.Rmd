---
title: "R 360 New Variables and Plots"
output: html_notebook
---
##Initial questions:
* **How well do coders handle their core responsibilities?**  
	* Severity
	* productivity
	* accuracy 

* **in what ways do coders contribute beyond their core responsibilities**
    - Coaching (peers, CDI, physicians)
    - peer review
    - querying
    - abstracting
    - harder to quantify with available data
* **how do you quantify a 'win' in hospital coding?**  I.e. how do you know whether the team (hospital/market/client) succeeding or failing?
    - in what ways are coders part of a team? 
    - May you don’t need 5 "Albert Pujols"" coders if you have one or two, and the others are playing key supporting roles — focus on team results. 
    - The team (facility) "win" is what counts. It's the hospital provider number, not the coder who is on the claim.
    - 

The challenge is quality vs quantity a trade off?  Triangle: severity, time, accuracy.   Is it zero sum, where you focus on one at the expense of the other two; Or can the best coder, trained in concepts and process, excell in all three?

**What do our coder query data say?**
    - **do some coders never generate queries?** 
        - *Descriptive*
        - Rate: Query_Rate = (Queries / Accounts coded) %>% identify lowest performers at facility and hub level
            - need accounts coded denominator from IP004
            - 09d will only display coders with at least 1 query / need to pull in all IP/Dx coders
        - Plot: 
        - df = query_rate_cdr = Coder | Queries | Accounts Coded | Query Rate
            - df = vol_cdr = Coder | Accounts coded
            
        - Query rate by hospital then identify lowest facility performers
        df = query_rate_fac = Facility | Queries | Accounts Coded | Query Rate
        - intervening: is the CDI program effective so coders do not need to query? - facility query rate should incorporate CDI queries
        
1. Exploratory Questions

**What are key distinctions between cdr/fac with higher vs lower query rates?**  
    - 

    -  **Is there a correlation between severity capture and querying**
        - *Exploratory*
        - Plot: Query_Rate vs average severity score (excluding OB and NB)
        - df = query_sev_cdr = Coder | Query Rate | mean(soi+rom)
            - sev_cdr = Coder | SOI | ROM | avg.sev <- from IP004
    - **Is there a correlation between higher precision and accuracy and higher querying?**
        - *exploratory*
        - Plot: Precision (CAC001) vs query rate
        - df = inner_join(CAC001_all_cdr, query_rate_cdr)
        - df = inner_join(CAC001_all_cdr, query_rate_fac)  

1. Inferential Questions
**What can we conclude by these distinctions? -- If a cdr/fac has a low query rate does that indiate a culture of not querying or is there some other factor?**
    - **In the aggregate, do coders who query more have lower productivity than coders who do not?**
        - Plot: queries (raw #) vs accounts coded
        - df = query_rate_cdr
    - **Are there any facilities that have a wide variation between coders who query and coders who do not?**
        - rate: sd(query_rate)
            - again need to include coders with 0 queries
1. Predictive Questions
At this stage we are not interested in predicting coder behavior.  Eventually this would something to be investigated.

1. Causal questions
**If a coder or facility were to improve query rates, would other metrics improve?**


# **CAC001** | auto|suggested codes precision and recall -------------------
1. Purpose: detect variance and assess general effectiveness of the NLP engine. Low rates can be indicative of coder adoption or NLP tuning opportunities.

1. Descriptive Questions
**What do our coder and facility P&R data say?**
    - **identify high/low performers (cdr/fac) with P&R**
        - std report provides this
            
    - **Do some facilities appear to have a culture of not querying?**
        - *Descriptive*
        - Query rate by hospital then identify lowest facility performers
        df = query_rate_fac = Facility | Queries | Accounts Coded | Query Rate
        - intervening: is the CDI program effective so coders do not need to query? - facility query rate should incorporate CDI queries
        
1. Exploratory Questions

**What are key distinctions between cdr/fac with higher vs lower query rates?**  
    - **Is there a correlation between severity capture and querying**
        - *Exploratory*
        - Plot: Query_Rate vs average severity score (excluding OB and NB)
        - df = query_sev_cdr = Coder | Query Rate | mean(soi+rom)
            - sev_cdr = Coder | SOI | ROM | avg.sev <- from IP004
    - **Is there a correlation between higher precision and accuracy and higher querying?**
        - *exploratory*
        - Plot: Precision (CAC001) vs query rate
        - df = inner_join(CAC001_all_cdr, query_rate_cdr)
        - df = inner_join(CAC001_all_cdr, query_rate_fac)  

1. Inferential Questions
**What can we conclude by these distinctions?**
    - **In the aggregate, do coders who query more have lower productivity than coders who do not?**
        - Plot: queries (raw #) vs accounts coded
        - df = query_rate_cdr
    - **Are there any facilities that have a wide variation between coders who query and coders who do not?**
        - rate: sd(query_rate)
            - again need to include coders with 0 queries
1. Predictive Questions
At this stage we are not interested in predicting coder behavior.  Eventually this would something to be investigated.

1. Causal questions
**If a coder or facility were to improve query rates, would other metrics improve?**


1. Four new tables:
	    1. CAC001_ip_dx_all
	    2. CAC001_ip_px_all
CAC001_all_fac:
- All_Suggested_fac = sum of all auto-suggested codes at a facility
- Accepted_fac = sum of all accepted coded at a facility
- All_coded_fac = sum of total codes at a facility
- % Precision_fac = Accepted_fac / All suggested_fac
- % Recall_fac = Accepted_fac / All_coded_fac
    - Precision and Recall rates are also provided at the hub level
- 


# **CAC003** | auto-suggested codes precision and recall by code ---------------------
1. Purpose: Identify low percentage acceptance of certain codes.  Can be used to evaluate coder adoption or NLP engine performance opportunities
1. Four new tables:
    1. CAC003_ip_cdr_all
    2. CAC003_ip_fac_all


# **CAC007** All Codes Entry Method Summary ------------------------
1. Purpose: determine coder and facility CAC vs manual entry methods
    - This is a really exciting report!  We can get at coder adoption of CAC
1. eight new tables:
    1. CAC007_ip_dx_cdr_all
    2. CAC007_ip_dx_fac_all

	
# **CAC008** | Auto-suggested codes listing ------------------------------------
1. Purpose: This report can be used for researching why coders may be rejecting A/S codes
1. One new tables: CAC008_all

    
# **IP004** | Primary and Secondary DRG Listing ----------------------------------------------------------
1. Purpose: Coded account inforation including DRGs

1. Questions
    - **How do facility volumes compare?**
        - *Descriptive*
            - df = vol_cdr = Coder | Accounts coded
    - **Do IP4 stats measure up with Prod016 stats?**
        - *Descriptive*
        - df = Coder | Accounts Coded (Ip4) | Accounts Coded (P16)

# **Prod016** | Coder Productivity -----------------------

# References
* Leek, Jeffrey, PhD.  Professer Bloomberg School of Public Health, Johns Hopkins University.