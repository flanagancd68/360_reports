---
title: "R 360 New Variables and Plots"
output: html_notebook
---

# The Question
- In order to know what variables and visualizations we want to create, we need to first ask ourselves what questions are we trying to answer
    - A failure to do this can result in data overload and/or the recipient wondering what he/she is supposed to do with all this data
- There are six basic categories of questions^[Leek, "Types of Data Science Questions"]  
    1. Descriptive
        a. describe but not make decisions/interpret/infer
        b. How many coders / accounts / queries / codes?
        
    1. Exploratory
        a. look at data and find relationships
        b. drive future analysis to confirm
        c. not the final say - shouldn't be used for generalization or predicting
        d. correlation is not causation
            - e.g. relationship between shoe size and intellect - the data shows that people with a size 2 are less intelligent than those with size 9.  People with a size 2 are also children where as size 9 are adults!
        e. **Are there any relationships between querying and severity?**
        
    2. Inferential
        a. take a small number of observations and extrapolate/generalize
        b. commonly the goal of statistical analysis - we want to draw conclusions
        
    3. Predictive
        a. More challenging - use data on some objects to predict value of another object
        b. just because x predicts y doesn't mean that x causes y
        c. "prediction is hard; especially about the future."
        d. Nate Silver examples of predicting US elections
    1. Casual
        a. if you change one value, will it change anothe?
        b. **if we improve P&R or the other metrics, will it improve ROI on CAC investement** (generally in terms of improved severity capture and coder productivity)?
        c. This is the **goal** of anlysis
    1. Mechanistic
        a. understand exact changes in variables that will lead to changes in other variables
        
- All the data in the world can't save you if you don't ask the right questions

# **09d** | Physician query listing -------------------------------------------
1. Purpose: detect query pattern variance among coders
1. Questions
    - **do some coders never generate queries?** 
        - *Descriptive*
        - Rate: Query_Rate = (Queries / Accounts coded) %>% identify lowest performers at facility and hub level
            - need accounts coded denominator from IP004
            - 09d will only display coders with at least 1 query / need to pull in all IP/Dx coders
        - Plot: 
        - df = query_rate_cdr = Coder | Queries | Accounts Coded | Query Rate
            - df = vol_cdr = Coder | Accounts coded

    - **Is there a correlation between severity capture and querying**
        - *Exploratory*
        - Plot: Query_Rate vs average severity score (excluding OB and NB)
        - df = query_sev_cdr = Coder | Query Rate | mean(soi+rom)
            - sev_cdr = Coder | SOI | ROM | avg.sev <- from IP004

    - **In the aggregate, do coders who query more have lower productivity than coders who do not?**
        - *Predictive* 
        - Plot: queries (raw #) vs accounts coded
        - df = query_rate_cdr

    - **Is there a correlation between higher precision and accuracy and higher querying?**
        - *exploratory*
        - Plot: Precision (CAC001) vs query rate
        - df = inner_join(CAC001_all_cdr, query_rate_cdr)

    - **Do some facilities appear to have a culture of not querying?**
        - *Descriptive*
        - Query rate by hospital then identify lowest facility performers
        df = query_rate_fac = Facility | Queries | Accounts Coded | Query Rate

    - **Are there any facilities that have a wide variation between coders who query and coders who do not?**
        - rate: sd(query_rate)
            - again need to include coders with 0 queries


# **CAC001** | auto|suggested codes precision and recall -------------------
1. Purpose: detect variance and assess general effectiveness of the NLP engine. Low rates can be indicative of coder adoption or NLP tuning opportunities.
1. Four new tables:
	    1. CAC001_ip_dx_all
	    2. CAC001_ip_px_all
CAC001_all_fac:
- All_Suggested_fac = sum of all auto-suggested codes at a facility
- Accepted_fac = sum of all accepted coded at a facility
- All_coded_fac = sum of total codes at a facility
- % Precision_fac = Accepted_fac / All suggested_fac
- % Recall_fac = Accepted_fac / All_coded_fac
    - Precision and Recall rates are also provided at the hub level
- 


# **CAC003** | auto-suggested codes precision and recall by code ---------------------
1. Purpose: Identify low percentage acceptance of certain codes.  Can be used to evaluate coder adoption or NLP engine performance opportunities
1. Four new tables:
    1. CAC003_ip_cdr_all
    2. CAC003_ip_fac_all


# **CAC007** All Codes Entry Method Summary ------------------------
1. Purpose: determine coder and facility CAC vs manual entry methods
    - This is a really exciting report!  We can get at coder adoption of CAC
1. eight new tables:
    1. CAC007_ip_dx_cdr_all
    2. CAC007_ip_dx_fac_all

	
# **CAC008** | Auto-suggested codes listing ------------------------------------
1. Purpose: This report can be used for researching why coders may be rejecting A/S codes
1. One new tables: CAC008_all

    
# **IP004** | Primary and Secondary DRG Listing ----------------------------------------------------------
1. Purpose: Coded account inforation including DRGs

1. Questions
    - **How do facility volumes compare?**
        - *Descriptive*
            - df = vol_cdr = Coder | Accounts coded
    - **Do IP4 stats measure up with Prod016 stats?**
        - *Descriptive*
        - df = Coder | Accounts Coded (Ip4) | Accounts Coded (P16)

# **Prod016** | Coder Productivity -----------------------

# References
* Leek, Jeffrey, PhD.  Professer Bloomberg School of Public Health, Johns Hopkins University.